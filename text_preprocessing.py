# -*- coding: utf-8 -*-
"""Text-Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q31x9LtqIlAhDoRGps116ACIGWpuceuT

## Dataset:
https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
"""

import os
import pandas as pd 
os.getcwd()

from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/MyDrive/Classes/NLP/data/IMDB Dataset.csv"

df = pd.read_csv(data_path)

df.shape

df = df.head(1000)

df.shape

df.head()

df['review'][0]

"""# lower case"""

df['review'][3]

df['review'] = df['review'].str.lower()

df['review'][3]

"""# remove_html_tags"""

import re
def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return pattern.sub(r'', text)

text = "<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>"

remove_html_tags(text)

df['review'] = df['review'].apply(remove_html_tags)

df['review'][5]

"""# remove_url"""

def remove_url(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return pattern.sub(r'', text)

text1 = 'Check out my youtube https://www.youtube.com/dswithbappy'
text2 = 'Check out my linkedin https://www.linkedin.com/in/boktiarahmed73/'
text3 = 'Google search here www.google.com'
text4 = 'For data click https://www.kaggle.com/'

remove_url(text1)

"""# punctuation handling"""

import string,time
string.punctuation

exclude = string.punctuation

def remove_punc(text):
    for char in exclude:
        text = text.replace(char,'')
    return text

text = 'string. With. Punctuation?'

start = time.time()
print(remove_punc(text))
time1 = time.time() - start
print(time1*50000)

def remove_punc1(text):
    return text.translate(str.maketrans('', '', exclude))

start = time.time()
remove_punc1(text)
time2 = time.time() - start
print(time2*50000)

time1/time2

df['review'][5]

remove_punc1(df['review'][5])

"""# chat_conversion handle"""

chat_words = {
    'AFAIK':'As Far As I Know',
    'AFK':'Away From Keyboard',
    'ASAP':'As Soon As Possible'
}

def chat_conversion(text):
    new_text = []
    for w in text.split():
        if w.upper() in chat_words:
            new_text.append(chat_words[w.upper()])
        else:
            new_text.append(w)
    return " ".join(new_text)

chat_conversion('Do this work ASAP')

"""# incorrect_text handling"""

from textblob import TextBlob

incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'

textBlb = TextBlob(incorrect_text)

textBlb.correct().string

"""# stopwords"""

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

stopwords.words('english')

def remove_stopwords(text):
    new_text = []
    
    for word in text.split():
        if word in stopwords.words('english'):
            new_text.append('')
        else:
            new_text.append(word)
    # print(new_text)
    x = new_text[:]
    # print(x)
    new_text.clear()
    return " ".join(x)

remove_stopwords('probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times')

df.head()

df['review'].apply(remove_stopwords)

"""# remove_emoji handle"""

import re
def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

remove_emoji("Loved the movie. It was ðŸ˜˜ðŸ˜˜")

remove_emoji("Lmao ðŸ˜‚ðŸ˜‚")

!pip install emoji

import emoji
print(emoji.demojize('Python is ðŸ”¥'))

print(emoji.demojize('Loved the movie. It was ðŸ˜˜'))

"""# Tokenization

### 1. Using the split function
"""

# word tokenization
sent1 = 'I am going to delhi'
sent1.split()

# sentence tokenization
sent2 = 'I am going to delhi. I will stay there for 3 days. Let\'s hope the trip to be great'
sent2.split('.')

# Problems with split function
sent3 = 'I am going to delhi!'
sent3.split()

sent4 = 'Where do think I should go? I have 3 day holiday'
sent4.split('.')

"""### 2. Regular Expression"""

import re
sent3 = 'I am going to delhi!'
tokens = re.findall("[\w']+", sent3)
tokens

text = """Lorem Ipsum is simply dummy text of the printing and typesetting industry? 
Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, 
when an unknown printer took a galley of type and scrambled it to make a type specimen book."""
sentences = re.compile('[.!?] ').split(text)
sentences

"""### 3. NLTK"""

from nltk.tokenize import word_tokenize,sent_tokenize
import nltk
nltk.download('punkt')

sent1 = 'I am going to visit delhi!'
word_tokenize(sent1)

text = """Lorem Ipsum is simply dummy text of the printing and typesetting industry? 
Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, 
when an unknown printer took a galley of type and scrambled it to make a type specimen book."""

sent_tokenize(text)

sent5 = 'I have a Ph.D in A.I'
sent6 = "We're here to help! mail us at nks@gmail.com"
sent7 = 'A 5km ride cost $10.50'

word_tokenize(sent5)
word_tokenize(sent6)
word_tokenize(sent7)

word_tokenize(sent6)

word_tokenize(sent7)

"""### 4. Spacy (good)"""

import spacy
nlp = spacy.load('en_core_web_sm')

doc1 = nlp(sent5)
doc2 = nlp(sent6)
doc3 = nlp(sent7)
doc4 = nlp(sent1)

for token in doc2:
    print(token)

"""# Stemmer"""

from nltk.stem.porter import PorterStemmer

ps = PorterStemmer()
def stem_words(text):
    return " ".join([ps.stem(word) for word in text.split()])

sample = "walk walks walking walked"
stem_words(sample)

text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'
print(text)

stem_words(text)

"""# Lemmatization"""

import nltk
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
wordnet_lemmatizer = WordNetLemmatizer()

sentence = "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun."
punctuations="?:!.,;"
sentence_words = nltk.word_tokenize(sentence)
for word in sentence_words:
    if word in punctuations:
        sentence_words.remove(word)

sentence_words
print("{0:20}{1:20}".format("Word","Lemma"))
for word in sentence_words:
    print ("{0:20}{1:20}".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))

"""#### NOTE: Stemming & lamatization are same to retrieve root words but lamatization is better. Lamatization is slow & stemming is fast"""

